{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import mnist, CIFAR10\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import torch.nn.functional as TF\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 12)\n",
      "(84, 12)\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('data/iyer/iyer.txt')[1:]\n",
    "flag_line = data[:,1]\n",
    "# print(flag_line)\n",
    "neg1_list = []\n",
    "for i in range(len(flag_line)):\n",
    "    if (data[i,1] == -1):\n",
    "        neg1_list.append(i)\n",
    "data = np.delete(data, neg1_list, axis=0)\n",
    "\n",
    "all_x = data[:,2:]\n",
    "all_y = data[:,1] - 1\n",
    "\n",
    "all_idx = np.arange(len(all_x))\n",
    "SEED_train_test = 1\n",
    "np.random.seed(SEED_train_test)\n",
    "np.random.shuffle(all_idx)\n",
    "tr_len= 400\n",
    "x_tr = all_x[all_idx[:tr_len]]\n",
    "y_tr = all_y[all_idx[:tr_len]]\n",
    "\n",
    "x_te = all_x[all_idx[tr_len:]]\n",
    "y_te = all_y[all_idx[tr_len:]]\n",
    "\n",
    "print(x_tr.shape)\n",
    "print(x_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_index:0,Accuracy Score:80.0000%\n",
      "K_index:1,Accuracy Score:80.0000%\n",
      "K_index:2,Accuracy Score:75.0000%\n",
      "K_index:3,Accuracy Score:77.5000%\n",
      "K_index:4,Accuracy Score:86.2500%\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'Models/'\n",
    "K_FOLD = 5\n",
    "kf = KFold(n_splits=K_FOLD)\n",
    "k_idx = 0\n",
    "for train_index, val_index in kf.split(x_tr):\n",
    "    train_x = x_tr[train_index]\n",
    "    train_y = y_tr[train_index]\n",
    "    val_x = x_tr[val_index]\n",
    "    val_y = y_tr[val_index]\n",
    "    # Train model\n",
    "    rf_clf = RandomForestClassifier(random_state=42)\n",
    "    rf_clf.fit(train_x, train_y)\n",
    "    \n",
    "    save_path = SAVE_DIR + 'IYRE_RF_K_%d.pkl'%k_idx\n",
    "    pickle.dump(rf_clf, open(save_path, 'wb'))\n",
    "    \n",
    "    rf_val_clf = pickle.load(open(save_path, 'rb'))\n",
    "    \n",
    "    y_pred = rf_val_clf.predict(val_x)\n",
    "    score = metrics.accuracy_score(val_y, y_pred)\n",
    "    print('K_index:%d,Accuracy Score:%.4f%%'%(k_idx,100*score))\n",
    "    k_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_idx:0\n",
      "Accuracy Score: 1.0\n",
      "F1 Score: 1.0\n",
      "ROC AUC Score: 1.0\n",
      "Average score:1.0000\n",
      "K_idx:1\n",
      "Accuracy Score: 1.0\n",
      "F1 Score: 1.0\n",
      "ROC AUC Score: 1.0\n",
      "Average score:1.0000\n",
      "K_idx:2\n",
      "Accuracy Score: 1.0\n",
      "F1 Score: 1.0\n",
      "ROC AUC Score: 1.0\n",
      "Average score:1.0000\n",
      "K_idx:3\n",
      "Accuracy Score: 1.0\n",
      "F1 Score: 1.0\n",
      "ROC AUC Score: 1.0\n",
      "Average score:1.0000\n",
      "K_idx:4\n",
      "Accuracy Score: 0.8625\n",
      "F1 Score: 0.8606734639629374\n",
      "ROC AUC Score: 0.9833333333333334\n",
      "Average score:0.9022\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'Models/'\n",
    "K_FOLD = 5\n",
    "kf = KFold(n_splits=K_FOLD)\n",
    "k_idx = 0\n",
    "for train_index, val_index in kf.split(x_tr):\n",
    "    val_x = x_tr[val_index]\n",
    "    val_y = y_tr[val_index]\n",
    "    val_y_onehot = np.eye(10)[val_y.astype(int)]\n",
    "    \n",
    "    \n",
    "    save_path = SAVE_DIR + 'IYRE_RF_K_%d.pkl'%k_idx\n",
    "    pickle.dump(rf_clf, open(save_path, 'wb'))\n",
    "    \n",
    "    rf_val_clf = pickle.load(open(save_path, 'rb'))\n",
    "    \n",
    "    y_pred = rf_val_clf.predict(val_x)\n",
    "    y_prob = rf_val_clf.predict_proba(val_x)\n",
    "    \n",
    "    score = metrics.accuracy_score(val_y, y_pred)\n",
    "    F1_score = metrics.f1_score(val_y, y_pred, average='weighted')\n",
    "    ROC_AUC_score = metrics.roc_auc_score(val_y_onehot, y_prob,average='samples',multi_class='ovo')\n",
    "    print('K_idx:%d'%(k_idx))\n",
    "    print('Accuracy Score:', score)\n",
    "    print('F1 Score:', F1_score)\n",
    "    print('ROC AUC Score:', ROC_AUC_score)\n",
    "    print('Average score:%.4f'%((score+F1_score+ROC_AUC_score)/3))\n",
    "    k_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_idx:4\n",
      "Accuracy Score: 0.8333333333333334\n",
      "F1 Score: 0.8249482280637297\n",
      "ROC AUC Score: 0.9761904761904762\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'Models/'\n",
    "k_idx = 4\n",
    "\n",
    "y_te_onehot = np.eye(10)[y_te.astype(int)]\n",
    "save_path = SAVE_DIR + 'IYRE_RF_K_%d.pkl'%k_idx\n",
    "\n",
    "rf_val_clf = pickle.load(open(save_path, 'rb'))\n",
    "\n",
    "y_pred = rf_val_clf.predict(x_te)\n",
    "y_prob = rf_val_clf.predict_proba(x_te)\n",
    "\n",
    "score = metrics.accuracy_score(y_te, y_pred)\n",
    "F1_score = metrics.f1_score(y_te, y_pred, average='weighted')\n",
    "ROC_AUC_score = metrics.roc_auc_score(y_te_onehot, y_prob,average='samples',multi_class='ovo')\n",
    "print('K_idx:%d'%(k_idx))\n",
    "print('Accuracy Score:', score)\n",
    "print('F1 Score:', F1_score)\n",
    "print('ROC AUC Score:', ROC_AUC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
