{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch.nn.functional as TF\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
    "])\n",
    "\n",
    "train_batch_size = 512\n",
    "val_batch_size = 250\n",
    "test_batch_size = 100\n",
    "\n",
    "train_dataset = CIFAR10(root='data/CIFAR10/train', train=True,\n",
    "                              download=True,transform=transform)\n",
    "test_dataset = CIFAR10(root='data/CIFAR10/test', train=False,\n",
    "                             download=True, transform=test_transform)\n",
    "\n",
    "\n",
    "len(train_dataset)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=10),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN5, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3,64,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64))\n",
    "        self.pool =nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(64,128,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128))\n",
    "        self.conv3 = nn.Sequential(\n",
    "                        nn.Conv2d(128,128,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128))\n",
    "        self.classifier = nn.Sequential(\n",
    "                            nn.Linear(128*4*4, 512),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm1d(512),\n",
    "                            nn.Dropout(0.2),\n",
    "                            nn.Linear(512, 512),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm1d(512),\n",
    "                            nn.Linear(512,10)\n",
    "                            )\n",
    "    def forward(self,x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        x = x.view((-1,128*4*4))\n",
    "        return self.classifier(x)\n",
    "#         return TF.softmax(self.fc2(x),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, idx:0, loss:2.475192\n",
      "Epoch:0, idx:50, loss:1.401187\n",
      "Validation accuracy: 54.8200\n",
      "Epoch:1, idx:0, loss:1.196840\n",
      "Epoch:1, idx:50, loss:1.103856\n",
      "Validation accuracy: 62.4800\n",
      "Epoch:2, idx:0, loss:0.928010\n",
      "Epoch:2, idx:50, loss:0.827095\n",
      "Validation accuracy: 68.9400\n",
      "Epoch:3, idx:0, loss:0.739573\n",
      "Epoch:3, idx:50, loss:0.677014\n",
      "Validation accuracy: 71.9600\n",
      "Epoch:4, idx:0, loss:0.573635\n",
      "Epoch:4, idx:50, loss:0.697014\n",
      "Validation accuracy: 75.1400\n",
      "Epoch:5, idx:0, loss:0.481189\n",
      "Epoch:5, idx:50, loss:0.322636\n",
      "Validation accuracy: 78.2800\n",
      "Epoch:6, idx:0, loss:0.318092\n",
      "Epoch:6, idx:50, loss:0.293280\n",
      "Validation accuracy: 78.7000\n",
      "Epoch:7, idx:0, loss:0.215598\n",
      "Epoch:7, idx:50, loss:0.231884\n",
      "Validation accuracy: 78.0200\n",
      "Epoch:8, idx:0, loss:0.258276\n",
      "Epoch:8, idx:50, loss:0.210745\n",
      "Validation accuracy: 78.7400\n",
      "Epoch:9, idx:0, loss:0.194529\n",
      "Epoch:9, idx:50, loss:0.167714\n",
      "Validation accuracy: 78.5000\n",
      "Epoch:0, idx:0, loss:2.422695\n",
      "Epoch:0, idx:50, loss:1.365348\n",
      "Validation accuracy: 56.7200\n",
      "Epoch:1, idx:0, loss:1.141451\n",
      "Epoch:1, idx:50, loss:1.180336\n",
      "Validation accuracy: 64.4400\n",
      "Epoch:2, idx:0, loss:0.913092\n",
      "Epoch:2, idx:50, loss:0.907914\n",
      "Validation accuracy: 71.5600\n",
      "Epoch:3, idx:0, loss:0.704651\n",
      "Epoch:3, idx:50, loss:0.639622\n",
      "Validation accuracy: 72.8800\n",
      "Epoch:4, idx:0, loss:0.543045\n",
      "Epoch:4, idx:50, loss:0.588172\n",
      "Validation accuracy: 75.7600\n",
      "Epoch:5, idx:0, loss:0.440812\n",
      "Epoch:5, idx:50, loss:0.329313\n",
      "Validation accuracy: 78.6800\n",
      "Epoch:6, idx:0, loss:0.261543\n",
      "Epoch:6, idx:50, loss:0.247297\n",
      "Validation accuracy: 79.6400\n",
      "Epoch:7, idx:0, loss:0.246232\n",
      "Epoch:7, idx:50, loss:0.222174\n",
      "Validation accuracy: 80.0600\n",
      "Epoch:8, idx:0, loss:0.206205\n",
      "Epoch:8, idx:50, loss:0.200367\n",
      "Validation accuracy: 79.3400\n",
      "Epoch:9, idx:0, loss:0.173180\n",
      "Epoch:9, idx:50, loss:0.216934\n",
      "Validation accuracy: 79.4000\n",
      "Epoch:0, idx:0, loss:2.402028\n",
      "Epoch:0, idx:50, loss:1.344626\n",
      "Validation accuracy: 53.9400\n",
      "Epoch:1, idx:0, loss:1.131174\n",
      "Epoch:1, idx:50, loss:1.128718\n",
      "Validation accuracy: 62.2000\n",
      "Epoch:2, idx:0, loss:0.896874\n",
      "Epoch:2, idx:50, loss:0.849249\n",
      "Validation accuracy: 69.8200\n",
      "Epoch:3, idx:0, loss:0.613444\n",
      "Epoch:3, idx:50, loss:0.763789\n",
      "Validation accuracy: 73.7400\n",
      "Epoch:4, idx:0, loss:0.559700\n",
      "Epoch:4, idx:50, loss:0.619706\n",
      "Validation accuracy: 72.8200\n",
      "Epoch:5, idx:0, loss:0.474883\n",
      "Epoch:5, idx:50, loss:0.384573\n",
      "Validation accuracy: 78.0800\n",
      "Epoch:6, idx:0, loss:0.299731\n",
      "Epoch:6, idx:50, loss:0.283055\n",
      "Validation accuracy: 78.0400\n",
      "Epoch:7, idx:0, loss:0.266907\n",
      "Epoch:7, idx:50, loss:0.253537\n",
      "Validation accuracy: 78.8400\n",
      "Epoch:8, idx:0, loss:0.242474\n",
      "Epoch:8, idx:50, loss:0.207425\n",
      "Validation accuracy: 77.9600\n",
      "Epoch:9, idx:0, loss:0.204252\n",
      "Epoch:9, idx:50, loss:0.211325\n",
      "Validation accuracy: 79.1000\n",
      "Epoch:0, idx:0, loss:2.437898\n",
      "Epoch:0, idx:50, loss:1.460456\n",
      "Validation accuracy: 55.1800\n",
      "Epoch:1, idx:0, loss:1.226976\n",
      "Epoch:1, idx:50, loss:1.158247\n",
      "Validation accuracy: 64.7000\n",
      "Epoch:2, idx:0, loss:0.940776\n",
      "Epoch:2, idx:50, loss:0.824083\n",
      "Validation accuracy: 70.2000\n",
      "Epoch:3, idx:0, loss:0.622172\n",
      "Epoch:3, idx:50, loss:0.652336\n",
      "Validation accuracy: 73.8600\n",
      "Epoch:4, idx:0, loss:0.484171\n",
      "Epoch:4, idx:50, loss:0.623548\n",
      "Validation accuracy: 75.2200\n",
      "Epoch:5, idx:0, loss:0.388000\n",
      "Epoch:5, idx:50, loss:0.360537\n",
      "Validation accuracy: 79.7400\n",
      "Epoch:6, idx:0, loss:0.319817\n",
      "Epoch:6, idx:50, loss:0.292851\n",
      "Validation accuracy: 80.1800\n",
      "Epoch:7, idx:0, loss:0.250420\n",
      "Epoch:7, idx:50, loss:0.236734\n",
      "Validation accuracy: 80.2600\n",
      "Epoch:8, idx:0, loss:0.257618\n",
      "Epoch:8, idx:50, loss:0.233426\n",
      "Validation accuracy: 80.5400\n",
      "Epoch:9, idx:0, loss:0.165202\n",
      "Epoch:9, idx:50, loss:0.235119\n",
      "Validation accuracy: 80.2800\n",
      "Epoch:0, idx:0, loss:2.425464\n",
      "Epoch:0, idx:50, loss:1.573476\n",
      "Validation accuracy: 53.9800\n",
      "Epoch:1, idx:0, loss:1.171278\n",
      "Epoch:1, idx:50, loss:1.113011\n",
      "Validation accuracy: 62.3200\n",
      "Epoch:2, idx:0, loss:0.928466\n",
      "Epoch:2, idx:50, loss:1.023009\n",
      "Validation accuracy: 68.9000\n",
      "Epoch:3, idx:0, loss:0.749886\n",
      "Epoch:3, idx:50, loss:0.690780\n",
      "Validation accuracy: 72.2400\n",
      "Epoch:4, idx:0, loss:0.549644\n",
      "Epoch:4, idx:50, loss:0.614629\n",
      "Validation accuracy: 75.1000\n",
      "Epoch:5, idx:0, loss:0.435436\n",
      "Epoch:5, idx:50, loss:0.364028\n",
      "Validation accuracy: 77.6600\n",
      "Epoch:6, idx:0, loss:0.357650\n",
      "Epoch:6, idx:50, loss:0.300481\n",
      "Validation accuracy: 78.8400\n",
      "Epoch:7, idx:0, loss:0.263925\n",
      "Epoch:7, idx:50, loss:0.244771\n",
      "Validation accuracy: 78.7600\n",
      "Epoch:8, idx:0, loss:0.240654\n",
      "Epoch:8, idx:50, loss:0.228364\n",
      "Validation accuracy: 78.7400\n",
      "Epoch:9, idx:0, loss:0.207999\n",
      "Epoch:9, idx:50, loss:0.178333\n",
      "Validation accuracy: 78.9800\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = 'Logs/'\n",
    "SAVE_DIR = 'Models/'\n",
    "for SEED in range(5):\n",
    "    torch.manual_seed(SEED)\n",
    "    train, val = random_split(train_dataset,[int(0.9*len(train_dataset)),int(0.1*len(train_dataset))])\n",
    "    \n",
    "#     all_index = np.arange(len(train_dataset))\n",
    "#     np.random.shuffle(all_index)\n",
    "#     train_index = all_index[0:int(0.9*len(train_dataset))]\n",
    "#     val_index = all_index[int(0.9*len(train_dataset)):]\n",
    "    \n",
    "    train_loader = DataLoader(train, shuffle=True, batch_size=train_batch_size)\n",
    "    val_loader = DataLoader(val, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "#     model = LeNet5().double().to(device)\n",
    "    model = CNN5().double().to(device)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    LR_STEP = [5]\n",
    "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,LR_STEP,gamma=0.1)\n",
    "\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    epoch = 10\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    LEARN_SEED = 42\n",
    "    torch.manual_seed(LEARN_SEED)\n",
    "    best_val_acc = 0.0\n",
    "    for _epoch in range(epoch):\n",
    "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "            train_x, train_label = train_x.double().to(device), train_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_x)\n",
    "            loss = cost(outputs, train_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if idx % 50 == 0:\n",
    "                print('Epoch:%d, idx:%d, loss:%.6f'%(_epoch, idx, loss.sum().item()))\n",
    "            train_loss.append(loss.sum().item())\n",
    "        train_scheduler.step()\n",
    "        correct = 0\n",
    "        _sum = 0\n",
    "\n",
    "        for idx, (val_x, val_label) in enumerate(val_loader):\n",
    "            val_x, val_label = val_x.double().to(device), val_label.to(device)\n",
    "            outputs = model(val_x).detach()\n",
    "            t_loss = cost(outputs, val_label)\n",
    "            predict_ys = torch.argmax(outputs, axis=-1)\n",
    "            _ = predict_ys.detach().data == val_label\n",
    "            correct += torch.sum(_, axis=-1)\n",
    "            _sum += _.shape[0]\n",
    "            val_loss.append(t_loss.sum().item())\n",
    "        val_acc = 100*correct / _sum\n",
    "        print('Validation accuracy: %.4f'%val_acc)\n",
    "    \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_ckpt = {'net':model.state_dict(),\n",
    "                        'optim':optimizer.state_dict(),\n",
    "                        'epoch':_epoch,\n",
    "                        'val_acc':best_val_acc}\n",
    "            best_save_path = SAVE_DIR + \"CIFAR10_CNN_Val_SEED_%d_model\"%SEED\n",
    "            torch.save(best_ckpt, best_save_path)\n",
    "            \n",
    "    log_save_path = LOG_DIR + \"CIFAR10_CNN_Val_SEED_%d_log\"%SEED\n",
    "\n",
    "    pickle.dump([train_loss, val_loss], open(log_save_path,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(78.7400, device='cuda:0')\n",
      "tensor(80.0600, device='cuda:0')\n",
      "tensor(79.1000, device='cuda:0')\n",
      "tensor(80.5400, device='cuda:0')\n",
      "tensor(78.9800, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for SEED in range(5):\n",
    "    best_save_path = SAVE_DIR + \"CIFAR10_CNN_Val_SEED_%d_model\"%SEED\n",
    "    print(torch.load(best_save_path)['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79\n",
      "0.7898787064510552\n",
      "[0.98192247 0.9935524  0.95589057 0.93673521 0.97084615 0.95590862\n",
      " 0.98503009 0.98338936 0.99261239 0.9901994 ]\n",
      "0.791\n",
      "0.7906085315219561\n",
      "[0.98582368 0.99390561 0.96179778 0.94075977 0.96608717 0.9631363\n",
      " 0.98476434 0.98613404 0.99359354 0.98529813]\n",
      "0.789\n",
      "0.7892196434579926\n",
      "[0.98204814 0.99379228 0.96010843 0.93952928 0.96956999 0.95580408\n",
      " 0.98395131 0.98492237 0.99323413 0.98994507]\n",
      "0.799\n",
      "0.7995330113189877\n",
      "[0.9811738  0.99455143 0.96426019 0.95355115 0.97593204 0.9613186\n",
      " 0.98549902 0.98241566 0.99077053 0.99405649]\n",
      "0.7856\n",
      "0.7858755251175678\n",
      "[0.98311079 0.99143665 0.95706432 0.93999018 0.9701727  0.95863125\n",
      " 0.98531294 0.98415577 0.99206875 0.98487129]\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'Models/'\n",
    "for SEED in range(5):\n",
    "    torch.manual_seed(SEED)\n",
    "    train, val = random_split(train_dataset,[int(0.9*len(train_dataset)),int(0.1*len(train_dataset))])\n",
    "    \n",
    "    train_loader = DataLoader(train, shuffle=True, batch_size=train_batch_size)\n",
    "    val_loader = DataLoader(val, shuffle=True, batch_size=val_batch_size)\n",
    "    \n",
    "    best_save_path = SAVE_DIR + \"CIFAR10_CNN_Val_SEED_%d_model\"%SEED\n",
    "    model = CNN5().double().to(device)\n",
    "    model.load_state_dict(torch.load(best_save_path)['net'])\n",
    "    \n",
    "    y_vals = []\n",
    "    y_vals_onehot = []\n",
    "    y_outputs = []\n",
    "    y_preds = []\n",
    "    for idx, (val_x, val_label) in enumerate(val_loader):\n",
    "        val_x, val_label = val_x.double().to(device), val_label.to(device)\n",
    "        y_vals.append(val_label.cpu().data)\n",
    "        y_vals_onehot.append(TF.one_hot(val_label.cpu().data,10).numpy())\n",
    "        outputs = model(val_x).detach()\n",
    "        y_output = TF.softmax(outputs,-1)\n",
    "        y_outputs.append(y_output.detach().cpu().data.numpy())\n",
    "        y_pred = torch.argmax(outputs, axis=-1)\n",
    "        y_preds.append(y_pred.detach().cpu().data.numpy())\n",
    "        \n",
    "    y_vals = torch.stack(y_vals,0).numpy()\n",
    "    y_vals = np.array(y_vals).reshape([-1,1])\n",
    "    y_vals_onehot = np.eye(10)[y_vals].reshape([-1,10])\n",
    "    y_preds = np.array(y_preds).reshape([-1,1])\n",
    "    y_outputs = np.array(y_outputs).reshape([-1,10])\n",
    "    print(metrics.accuracy_score(y_vals,y_preds))\n",
    "    print(metrics.f1_score(y_vals,y_preds,average='weighted'))\n",
    "    print(metrics.roc_auc_score(y_vals_onehot,y_outputs,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782\n",
      "0.7816714609262266\n",
      "[0.97953433 0.99224767 0.95125511 0.93657278 0.97004967 0.95776578\n",
      " 0.98333356 0.98494644 0.98996589 0.990095  ]\n"
     ]
    }
   ],
   "source": [
    "y_te = test_dataset.targets\n",
    "LOG_DIR = 'Logs/'\n",
    "SAVE_DIR = 'Models/'\n",
    "\n",
    "SEED = 3\n",
    "best_save_path = SAVE_DIR + \"CIFAR10_CNN_Val_SEED_%d_model\"%SEED\n",
    "model = CNN5().double().to(device)\n",
    "model.load_state_dict(torch.load(best_save_path)['net'])\n",
    "y_vals = []\n",
    "y_vals_onehot = []\n",
    "y_outputs = []\n",
    "y_preds = []\n",
    "for idx, (val_x, val_label) in enumerate(test_loader):\n",
    "    val_x, val_label = val_x.double().to(device), val_label.to(device)\n",
    "    y_vals.append(val_label.cpu().data)\n",
    "    y_vals_onehot.append(TF.one_hot(val_label.cpu().data,10).numpy())\n",
    "    outputs = model(val_x).detach()\n",
    "    y_output = TF.softmax(outputs,-1)\n",
    "    y_outputs.append(y_output.detach().cpu().data.numpy())\n",
    "    y_pred = torch.argmax(outputs, axis=-1)\n",
    "    y_preds.append(y_pred.detach().cpu().data.numpy())\n",
    "\n",
    "y_vals = torch.stack(y_vals,0).numpy()\n",
    "y_vals = np.array(y_vals).reshape([-1,1])\n",
    "y_vals_onehot = np.eye(10)[y_vals].reshape([-1,10])\n",
    "y_preds = np.array(y_preds).reshape([-1,1])\n",
    "y_outputs = np.array(y_outputs).reshape([-1,10])\n",
    "print(metrics.accuracy_score(y_vals,y_preds))\n",
    "print(metrics.f1_score(y_vals,y_preds,average='weighted'))\n",
    "print(metrics.roc_auc_score(y_vals_onehot,y_outputs,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
